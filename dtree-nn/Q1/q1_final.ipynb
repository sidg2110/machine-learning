{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT AND ENCODE DATA <br>\n",
    "POSSIBLE ENCODERS - ordinal_encoder, one-hot_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_data(file_name, encoder):\n",
    "    data = pd.read_csv(file_name)\n",
    "    examples = data.shape[0]\n",
    "    \n",
    "    encoded_labels = ['team','host','opp','month', 'day_match']\n",
    "    remaining_labels = [\"year\",\"toss\",\"bat_first\",\"format\" ,\"fow\",\"score\" ,\"rpo\" ,\"result\"]\n",
    "    \n",
    "    label_encoder = None\n",
    "    \n",
    "    if encoder == 'ordinal_encoder':\n",
    "        label_encoder = OrdinalEncoder()\n",
    "    elif encoder == 'one-hot_encoder':\n",
    "        label_encoder = OneHotEncoder(sparse_output = False)\n",
    "        \n",
    "    label_encoder.fit(data[encoded_labels])\n",
    "    \n",
    "    data_1 = pd.DataFrame(label_encoder.transform(data[encoded_labels]), columns = label_encoder.get_feature_names_out()).to_numpy()\n",
    "    data_2 = data[remaining_labels].to_numpy()\n",
    "\n",
    "    temp = np.full((examples, 2), 0.0)\n",
    "    \n",
    "    final_data = []\n",
    "    if encoder == 'ordinal_encoder':\n",
    "        final_data = np.concatenate((data_1, data_2), axis=1)\n",
    "    elif encoder == 'one-hot_encoder':\n",
    "        # adds two features for the values which were not included in the training data\n",
    "        final_data = np.concatenate((data_1, temp, data_2), axis=1)\n",
    "    \n",
    "    X = final_data[:, :-1]\n",
    "    Y = final_data[:, -1:]\n",
    "    \n",
    "    return X, Y.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET FEATURE INFORMATION <br>\n",
    "Feature type and number of categories <br>\n",
    "Generates an array of which features are available for split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_info(encoder):\n",
    "    \n",
    "    feature_type, feature_ct, feature_allowed = [], [], []\n",
    "\n",
    "    if encoder == 'ordinal_encoder':\n",
    "        feature_type = ['cat', 'cat', 'cat', 'cat', 'cat', 'cont', 'cat', 'cat', 'cat', 'cont', 'cont', 'cont']\n",
    "        feature_ct = [20, 19, 20, 12, 3, -1, 2, 2, 2, -1, -1, -1]\n",
    "        feature_allowed = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "    \n",
    "    elif encoder == 'one-hot_encoder':\n",
    "        encoded_ct = 74\n",
    "        feature_type = np.full((encoded_ct, ), 'cat')\n",
    "        feature_ct = np.full((encoded_ct, ), 2)\n",
    "\n",
    "        feature_temp_type = np.array(['cont', 'cat', 'cat', 'cat', 'cont', 'cont', 'cont'])\n",
    "        feature_temp_ct = np.array([-1, 2, 2, 2, -1, -1, -1])\n",
    "\n",
    "        feature_type = np.concatenate((feature_type, feature_temp_type))\n",
    "        feature_ct = np.concatenate((feature_ct, feature_temp_ct))\n",
    "        feature_allowed = np.full((feature_type.size, ), 1)\n",
    "\n",
    "    return feature_type, feature_ct, feature_allowed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA - ORDINAL ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_ordinal, train_y_ordinal = get_encoded_data('train.csv', 'ordinal_encoder')\n",
    "validation_x_ordinal, validation_y_ordinal = get_encoded_data('val.csv', 'ordinal_encoder')\n",
    "test_x_ordinal, test_y_ordinal = get_encoded_data('test.csv', 'ordinal_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DATA - ONE-HOT ENCODER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_one_hot, train_y_one_hot = get_encoded_data('train.csv', 'one-hot_encoder')\n",
    "validation_x_one_hot, validation_y_one_hot = get_encoded_data('val.csv', 'one-hot_encoder')\n",
    "test_x_one_hot, test_y_one_hot = get_encoded_data('test.csv', 'one-hot_encoder')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS FOR DECISION TREE NODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTNode:\n",
    "    def __init__(self):\n",
    "        self.id = 0\n",
    "        self.children = []\n",
    "        self.subtree_nodes = 1\n",
    "        self.subtree_leaf_nodes = 0\n",
    "        self.is_leaf = True\n",
    "        self.label_leaf = 0\n",
    "        self.label_if_leaf = 0\n",
    "        self.split_attr = None\n",
    "        self.median_value = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS FOR DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTree:\n",
    "    def __init__(self, train_x, train_y, max_depth, encoder):\n",
    "        \n",
    "        self.root = DTNode()\n",
    "        \n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.feature_type, self.feature_ct, self.feature_allowed = get_feature_info(encoder)\n",
    "        \n",
    "        self.nodes = 0\n",
    "        self.leaf_nodes = 0\n",
    "\n",
    "        # For Pruning\n",
    "        self.node_to_prune = None\n",
    "        self.best_accuracy = -1\n",
    "        self.ct = 0\n",
    "        self.make_decision_tree()\n",
    "\n",
    "    def get_entropy(self, y):\n",
    "        zeros, ones = len(y[y==0]), len(y[y==1])\n",
    "        if (zeros==0 or ones==0):\n",
    "            return 0\n",
    "        zero_prob, one_prob = zeros/(ones+zeros), ones/(ones+zeros)\n",
    "        entropy = -(zero_prob * np.log(zero_prob) + one_prob*np.log(one_prob))\n",
    "        return entropy\n",
    "\n",
    "    def get_cond_entropy(self, x, y, feature):\n",
    "        feature_row = x.T[feature]\n",
    "        entropy = 0\n",
    "\n",
    "        if self.feature_type[feature] == 'cat':\n",
    "            categories = np.unique(feature_row)\n",
    "            for category in categories:\n",
    "                prob_category = np.sum(feature_row == category) / len(feature_row)\n",
    "                entropy_given_category = self.get_entropy(y[feature_row == category])\n",
    "                entropy += prob_category * entropy_given_category\n",
    "        \n",
    "        elif self.feature_type[feature] == 'cont':\n",
    "            median = np.median(feature_row)\n",
    "            # less than equal to\n",
    "            prob_less = np.sum(feature_row <= median) / len(feature_row)\n",
    "            entropy_given_less = self.get_entropy(y[feature_row <= median])\n",
    "            entropy += prob_less * entropy_given_less\n",
    "            # greater than\n",
    "            prob_more = np.sum(feature_row > median) / len(feature_row)\n",
    "            entropy_given_more = self.get_entropy(y[feature_row > median])\n",
    "            entropy += prob_more * entropy_given_more\n",
    "        \n",
    "        return entropy\n",
    "    \n",
    "    def get_mutual_information(self, x, y, feature):\n",
    "        entropy = self.get_entropy(y)\n",
    "        cond_entropy = self.get_cond_entropy(x, y, feature)\n",
    "        return (entropy - cond_entropy)\n",
    "    \n",
    "    def choose_best_attribute(self, x, y):\n",
    "        best_attribute = -1\n",
    "        max_mutual_information = -1\n",
    "        columns = x.shape[1]\n",
    "        for feature in range(columns):\n",
    "            if self.feature_allowed[feature] == 1:\n",
    "                mutual_information = self.get_mutual_information(x, y, feature)\n",
    "                if mutual_information > max_mutual_information:\n",
    "                    max_mutual_information = mutual_information\n",
    "                    best_attribute = feature\n",
    "        return best_attribute\n",
    "    \n",
    "    def split_data(self, x, y, feature):\n",
    "        data_x, data_y = [], []\n",
    "        feature_row = x.T[feature]\n",
    "        if self.feature_type[feature] == 'cat':\n",
    "            for i in range(self.feature_ct[feature]):\n",
    "                x_subset = x[feature_row == i]\n",
    "                y_subset = y[feature_row == i]\n",
    "                data_x.append(x_subset)\n",
    "                data_y.append(y_subset)\n",
    "        elif self.feature_type[feature] == 'cont':\n",
    "            median = np.median(feature_row)\n",
    "            x_less, y_less = x[feature_row <= median], y[feature_row <= median]\n",
    "            x_more, y_more = x[feature_row > median], y[feature_row > median]\n",
    "            data_x.append(x_less)\n",
    "            data_x.append(x_more)\n",
    "            data_y.append(y_less)\n",
    "            data_y.append(y_more)\n",
    "\n",
    "        return (data_x, data_y)\n",
    "    \n",
    "    def get_max_label(self, y):\n",
    "        ones = y[y==1].size\n",
    "        zeros = y[y==0].size\n",
    "        if ones > zeros:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def grow_decision_tree(self, node, x, y, depth_allowed):\n",
    "        \n",
    "        node.id = self.ct\n",
    "        self.ct += 1\n",
    "        \n",
    "        self.nodes += 1\n",
    "        \n",
    "        label_if_leaf = self.get_max_label(y)\n",
    "        node.label_if_leaf = label_if_leaf\n",
    "                \n",
    "        if (depth_allowed == 0):\n",
    "            self.leaf_nodes += 1\n",
    "            # node.subtree_nodes += 1\n",
    "            # node.subtree_leaf_nodes += 1\n",
    "            node.is_leaf = True\n",
    "            node.label_leaf = label_if_leaf\n",
    "            return\n",
    "        \n",
    "        if (np.all(y==0) or np.all(y==1)):\n",
    "            self.leaf_nodes += 1\n",
    "            # node.subtree_nodes += 1\n",
    "            # node.subtree_leaf_nodes += 1\n",
    "            node.is_leaf = True\n",
    "            node.label_leaf = y[0]\n",
    "            return\n",
    "\n",
    "        node.is_leaf = False\n",
    "        \n",
    "        split_attr = self.choose_best_attribute(x, y)\n",
    "        \n",
    "        if (split_attr == -1):\n",
    "            self.leaf_nodes += 1\n",
    "            # node.subtree_nodes += 1\n",
    "            # node.subtree_leaf_nodes += 1\n",
    "            node.is_leaf = True\n",
    "            node.label_leaf = label_if_leaf\n",
    "            return\n",
    "        \n",
    "        node.split_attr = split_attr\n",
    "\n",
    "        if self.feature_type[split_attr] == 'cat':\n",
    "            self.feature_allowed[split_attr] = 0\n",
    "            \n",
    "        if self.feature_type[split_attr] == 'cont':\n",
    "            feature_row = x.T[split_attr]\n",
    "            median = np.median(feature_row)\n",
    "            node.median_value = median\n",
    "\n",
    "        data_x_list, data_y_list = self.split_data(x, y, split_attr)\n",
    "        children = len(data_x_list)\n",
    "        for i in range(children):\n",
    "            child = DTNode()\n",
    "            node.children.append(child)\n",
    "            if (data_y_list[i].size == 0):\n",
    "                self.nodes += 1\n",
    "                self.leaf_nodes += 1\n",
    "                node.subtree_nodes += 1\n",
    "                node.subtree_leaf_nodes += 1\n",
    "                child.is_leaf = True\n",
    "                child.label_leaf = label_if_leaf\n",
    "            else:\n",
    "                self.grow_decision_tree(child, data_x_list[i], data_y_list[i], depth_allowed-1)\n",
    "                node.subtree_nodes += child.subtree_nodes\n",
    "                node.subtree_leaf_nodes += child.subtree_leaf_nodes\n",
    "            self.feature_allowed[split_attr] = 1\n",
    "\n",
    "    def make_decision_tree(self):\n",
    "        self.grow_decision_tree(self.root, self.train_x, self.train_y, self.max_depth)\n",
    "        self.ct = 0\n",
    "        \n",
    "    def traverse_tree(self, node, x):\n",
    "        if node.is_leaf == True:\n",
    "            return node.label_leaf\n",
    "        \n",
    "        feature = node.split_attr\n",
    "        if self.feature_type[feature] == 'cat':\n",
    "            current_value = int(x[feature])\n",
    "            child = node.children[current_value]\n",
    "            return self.traverse_tree(child, x)\n",
    "        elif self.feature_type[feature] == 'cont':\n",
    "            current_value = x[feature]\n",
    "            split_val = node.median_value\n",
    "            child = None\n",
    "            if current_value <= split_val:\n",
    "                child = node.children[0]\n",
    "            else:\n",
    "                child = node.children[1]\n",
    "            return self.traverse_tree(child, x)\n",
    "        \n",
    "    def select_node_to_prune(self, node, x_val, y_val):\n",
    "        if (node.is_leaf == True):\n",
    "            return\n",
    "        node.is_leaf = True\n",
    "        node.label_leaf = node.label_if_leaf\n",
    "        new_accuracy = self.get_accuracy(x_val, y_val)\n",
    "        if new_accuracy > self.best_accuracy:\n",
    "            self.best_accuracy = new_accuracy\n",
    "            self.node_to_prune = node\n",
    "        node.is_leaf = False\n",
    "        for child in node.children:\n",
    "            self.select_node_to_prune(child, x_val, y_val)\n",
    "        return\n",
    "    \n",
    "    def prune(self, x_val, y_val, curr_accuracy):      \n",
    "        self.best_accuracy = -1\n",
    "        self.select_node_to_prune(self.root, x_val, y_val)\n",
    "        if (self.best_accuracy > curr_accuracy):\n",
    "            self.node_to_prune.is_leaf = True\n",
    "            self.node_to_prune.label_leaf = self.node_to_prune.label_if_leaf\n",
    "            self.nodes = (self.nodes) - (self.node_to_prune.subtree_nodes) + 1\n",
    "            self.leaf_nodes = (self.leaf_nodes) - (self.node_to_prune.subtree_leaf_nodes) + 1\n",
    "\n",
    "    def post_prune(self, x_val, y_val, x_train, y_train, x_test, y_test):\n",
    "        train_accuracy, test_accuracy, val_accuracy = [], [], []\n",
    "        nodes = []\n",
    "\n",
    "        while(True):\n",
    "            prev_val_accuracy = self.get_accuracy(x_val, y_val)\n",
    "            \n",
    "            train_accuracy.append(self.get_accuracy(x_train, y_train))\n",
    "            test_accuracy.append(self.get_accuracy(x_test, y_test))\n",
    "            val_accuracy.append(prev_val_accuracy)\n",
    "            nodes.append(self.nodes)\n",
    "            \n",
    "            self.prune(x_val, y_val, prev_val_accuracy)\n",
    "            \n",
    "            new_val_accuracy = self.get_accuracy(x_val, y_val)\n",
    "            \n",
    "            if ((new_val_accuracy - prev_val_accuracy) <= 1e-5):\n",
    "                train_accuracy.append(self.get_accuracy(x_train, y_train))\n",
    "                test_accuracy.append(self.get_accuracy(x_test, y_test))\n",
    "                val_accuracy.append(new_val_accuracy)\n",
    "                nodes.append(self.nodes)\n",
    "                break\n",
    "        \n",
    "        return (train_accuracy, val_accuracy, test_accuracy, nodes)\n",
    "\n",
    "    def get_accuracy(self, x, y):\n",
    "        predictions = []\n",
    "        for i in range(x.shape[0]):\n",
    "            prediction = self.traverse_tree(self.root, x[i])\n",
    "            predictions.append(prediction)\n",
    "        predictions = np.array(predictions)\n",
    "        accuracy = np.sum(predictions == y) / len(predictions)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTREE MODELS <br>\n",
    "Ordinal Encoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths_ordinal = [5, 10, 15, 20, 25]\n",
    "d_trees_ordinal = []\n",
    "train_accuracies_ordinal, test_accuracies_ordinal = [], []\n",
    "\n",
    "for max_depth in max_depths_ordinal:\n",
    "    d_tree = DTree(train_x_ordinal, train_y_ordinal, max_depth, 'ordinal_encoder')\n",
    "    train_accuracy = d_tree.get_accuracy(train_x_ordinal, train_y_ordinal)\n",
    "    test_accuracy = d_tree.get_accuracy(test_x_ordinal, test_y_ordinal)\n",
    "    d_trees_ordinal.append(d_tree)\n",
    "    train_accuracies_ordinal.append(train_accuracy)\n",
    "    test_accuracies_ordinal.append(test_accuracy)\n",
    "\n",
    "Y_only_win, Y_only_lose = np.ones(len(test_y_ordinal)), np.zeros(len(test_y_ordinal))\n",
    "only_win_accuracy = np.sum(Y_only_win == test_y_ordinal) / len(test_y_ordinal)\n",
    "only_lose_accuracy = np.sum(Y_only_lose == test_y_ordinal) / len(test_y_ordinal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train - {train_accuracies_ordinal}')\n",
    "print(f'test - {test_accuracies_ordinal}')\n",
    "print(f'win - {only_win_accuracy}')\n",
    "print(f'lose - {only_lose_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT FOR ACCURACIES VS DEPTH - ORDINAL ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths_ordinal, train_accuracies_ordinal, label='train', color='red')\n",
    "plt.plot(max_depths_ordinal, test_accuracies_ordinal, label='test', color='blue')\n",
    "\n",
    "plt.xlabel('Max Depths')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs Max Depth for Ordinal Encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTREE MODELS <br>\n",
    "One-Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_depths_one_hot = [15, 25, 35, 45]\n",
    "d_trees_one_hot = []\n",
    "train_accuracies_one_hot, test_accuracies_one_hot = [], []\n",
    "\n",
    "for max_depth in max_depths_one_hot:\n",
    "    d_tree = DTree(train_x_one_hot, train_y_one_hot, max_depth, 'one-hot_encoder')\n",
    "    train_accuracy = d_tree.get_accuracy(train_x_one_hot, train_y_one_hot)\n",
    "    test_accuracy = d_tree.get_accuracy(test_x_one_hot, test_y_one_hot)\n",
    "    d_trees_one_hot.append(d_tree)\n",
    "    train_accuracies_one_hot.append(train_accuracy)\n",
    "    test_accuracies_one_hot.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train - {train_accuracies_one_hot}')\n",
    "print(f'test - {test_accuracies_one_hot}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOT FOR ACCURACIES VS DEPTH - ONE-HOT ENCODING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(max_depths_one_hot, train_accuracies_one_hot, label='train', color='red')\n",
    "plt.plot(max_depths_one_hot, test_accuracies_one_hot, label='test', color='blue')\n",
    "\n",
    "plt.xlabel('Max Depths')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title('Accuracy vs Max Depth for One-Hot Encoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTREE MODELS <br>\n",
    "With Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_accuracies_list, val_accuracies_list, test_accuracies_list, nodes_list = [], [], [], []\n",
    "for d_tree in d_trees_one_hot:\n",
    "    result = d_tree.post_prune(validation_x_one_hot, validation_y_one_hot, train_x_one_hot, train_y_one_hot, test_x_one_hot, test_y_one_hot)\n",
    "    train_accuracies, val_accuracies, test_accuracies, nodes = result\n",
    "    train_accuracies_list.append(train_accuracies)\n",
    "    val_accuracies_list.append(val_accuracies)\n",
    "    test_accuracies_list.append(test_accuracies)\n",
    "    nodes_list.append(nodes)\n",
    "    print(f'Done : Initial = {nodes[0]} : Final = {nodes[-1]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'final-train-d_15-{train_accuracies_list[0][-1]}')\n",
    "print(f'final-valid-d_15-{val_accuracies_list[0][-1]}')\n",
    "print(f'final-test-d_15-{test_accuracies_list[0][-1]}')\n",
    "print(f'final-train-d_25-{train_accuracies_list[1][-1]}')\n",
    "print(f'final-valid-d_25-{val_accuracies_list[1][-1]}')\n",
    "print(f'final-test-d_25-{test_accuracies_list[1][-1]}')\n",
    "print(f'final-train-d_35-{train_accuracies_list[2][-1]}')\n",
    "print(f'final-valid-d_35-{val_accuracies_list[2][-1]}')\n",
    "print(f'final-test-d_35-{test_accuracies_list[2][-1]}')\n",
    "print(f'final-train-d_45-{train_accuracies_list[3][-1]}')\n",
    "print(f'final-valid-d_45-{val_accuracies_list[3][-1]}')\n",
    "print(f'final-test-d_45-{test_accuracies_list[3][-1]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS FOR ACCURACIES VS NUMBER OF NODES DURING POST-PRUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(max_depths_one_hot)):\n",
    "    plt.figure()\n",
    "    plt.plot(nodes_list[i], train_accuracies_list[i], label='train', color='red')\n",
    "    plt.plot(nodes_list[i], val_accuracies_list[i], label='validation', color='blue')\n",
    "    plt.plot(nodes_list[i], test_accuracies_list[i], label='test', color='green')\n",
    "    \n",
    "    plt.xlabel('Number of Nodes')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.title(f'Accuracy vs Nodes: Depth-{max_depths_one_hot[i]}')\n",
    "    plt.savefig(f'pruning-{max_depths_one_hot[i]}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS FOR DTREE USING SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DTree_Sklearn:\n",
    "    def __init__(self, train_x, train_y, max_depth=None, ccp_alpha=0.0):\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.max_depth = max_depth\n",
    "        self.ccp_alpha = ccp_alpha\n",
    "        self.decision_tree = self.make_decision_tree()\n",
    "\n",
    "    def make_decision_tree(self):\n",
    "        decision_tree = DecisionTreeClassifier(criterion='entropy', max_depth=self.max_depth, ccp_alpha=self.ccp_alpha)\n",
    "        decision_tree = decision_tree.fit(self.train_x, self.train_y)\n",
    "        return decision_tree\n",
    "\n",
    "    def get_accuracy(self, x, y):\n",
    "        predictions = self.decision_tree.predict(x)\n",
    "        accuracy = (np.sum(predictions == y.T)) / len(y)\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DTREE_SKLEARN MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths = [15, 25, 35, 45]\n",
    "ccp_alphas = [0.001, 0.01, 0.1, 0.2]\n",
    "\n",
    "train_accuracies_depth, validation_accuracies_depth, test_accuracies_depth = [], [], []\n",
    "train_accuracies_ccp, validation_accuracies_ccp, test_accuracies_ccp = [], [], []\n",
    "\n",
    "for depth in depths:\n",
    "    decision_tree_sklearn = DTree_Sklearn(train_x_one_hot, train_y_one_hot, depth, 0.0)\n",
    "    train_accuracy = decision_tree_sklearn.get_accuracy(train_x_one_hot, train_y_one_hot)\n",
    "    validation_accuracy = decision_tree_sklearn.get_accuracy(validation_x_one_hot, validation_y_one_hot)\n",
    "    test_accuracy = decision_tree_sklearn.get_accuracy(test_x_one_hot, test_y_one_hot)\n",
    "    train_accuracies_depth.append(train_accuracy)\n",
    "    validation_accuracies_depth.append(validation_accuracy)\n",
    "    test_accuracies_depth.append(test_accuracy)\n",
    "\n",
    "for ccp_alpha in ccp_alphas:\n",
    "    decision_tree_sklearn = DTree_Sklearn(train_x_one_hot, train_y_one_hot, None, ccp_alpha)\n",
    "    train_accuracy = decision_tree_sklearn.get_accuracy(train_x_one_hot, train_y_one_hot)\n",
    "    validation_accuracy = decision_tree_sklearn.get_accuracy(validation_x_one_hot, validation_y_one_hot)\n",
    "    test_accuracy = decision_tree_sklearn.get_accuracy(test_x_one_hot, test_y_one_hot)\n",
    "    train_accuracies_ccp.append(train_accuracy)\n",
    "    validation_accuracies_ccp.append(validation_accuracy)\n",
    "    test_accuracies_ccp.append(test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_depth = depths[np.argmax(validation_accuracies_depth)]\n",
    "best_ccp = ccp_alphas[np.argmax(validation_accuracies_ccp)]\n",
    "\n",
    "print(f'best depth = {best_depth}')\n",
    "print(f'best ccp_alpha = {best_ccp}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'train-depth-{train_accuracies_depth}')\n",
    "print(f'valid-depth-{validation_accuracies_depth}')\n",
    "print(f'test-depth-{test_accuracies_depth}')\n",
    "print(f'train-ccp-{train_accuracies_ccp}')\n",
    "print(f'valid-ccp-{validation_accuracies_ccp}')\n",
    "print(f'test-ccp-{test_accuracies_ccp}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tree_best = DTree_Sklearn(train_x_one_hot, train_y_one_hot, best_depth, best_ccp)\n",
    "best_train, best_val, best_test = d_tree_best.get_accuracy(train_x_one_hot, train_y_one_hot), d_tree_best.get_accuracy(validation_x_one_hot, validation_y_one_hot), d_tree_best.get_accuracy(test_x_one_hot, test_y_one_hot)\n",
    "\n",
    "print(f'train-{best_train}')\n",
    "print(f'val-{best_val}')\n",
    "print(f'test-{best_test}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS FOR ACCURACIES VS DEPTH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(depths, train_accuracies_depth, label='train', color='red')\n",
    "plt.plot(depths, validation_accuracies_depth, label='validation', color='blue')\n",
    "plt.plot(depths, test_accuracies_depth, label='test', color='green')\n",
    "\n",
    "plt.xlabel('Max Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(f'Accuracy vs Max Depth')\n",
    "plt.savefig(f'dt-sk-d.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTS FOR ACCURACIES VS PRUNING PARAMETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ccp_alphas, train_accuracies_ccp, label='train', color='red')\n",
    "plt.plot(ccp_alphas, validation_accuracies_ccp, label='validation', color='blue')\n",
    "plt.plot(ccp_alphas, test_accuracies_ccp, label='test', color='green')\n",
    "\n",
    "plt.xlabel('ccp_alpha')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(f'Accuracy vs ccp_alpha')\n",
    "plt.savefig(f'dt-sk-ccp.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLASS FOR RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Forest:\n",
    "    def __init__(self, train_x, train_y, n_estimators, max_features, min_samples_split):\n",
    "        \n",
    "        self.random_forest = None\n",
    "        self.train_x = train_x\n",
    "        self.train_y = train_y\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_features = max_features\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.make_random_forest()\n",
    "\n",
    "    def make_random_forest(self):\n",
    "        random_forest = RandomForestClassifier(n_estimators=self.n_estimators, criterion='entropy', max_features=self.max_features, min_samples_split=self.min_samples_split)\n",
    "        self.random_forest = random_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRID SEARCH FOR OPTIMAL PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_forests = {}\n",
    "\n",
    "n_estimators = [50, 150, 250, 350]\n",
    "max_features = [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "min_samples_splits = [2, 4, 6, 8, 10]\n",
    "\n",
    "best_oob_score = -1\n",
    "best_random_forest = None\n",
    "best_params = []\n",
    "for n_estimator in n_estimators:\n",
    "    for max_feature in max_features:\n",
    "        for min_samples_split in min_samples_splits:\n",
    "            random_forest = RandomForestClassifier(n_estimators=n_estimator, criterion='entropy', oob_score=True, max_features=max_feature, min_samples_split=min_samples_split)\n",
    "            random_forest.fit(train_x_one_hot, train_y_one_hot)\n",
    "            random_forests[(n_estimator, max_feature, min_samples_split)] = random_forest\n",
    "            \n",
    "            if (random_forest.oob_score_ > best_oob_score):\n",
    "                best_oob_score = random_forest.oob_score_\n",
    "                best_params = [n_estimator, max_feature, min_samples_split]\n",
    "                best_random_forest = random_forest\n",
    "            print(f'Done : e={n_estimator}, mf={max_feature}, mns={min_samples_split}')\n",
    "\n",
    "best_train_accuracy = np.sum(best_random_forest.predict(train_x_one_hot) == train_y_one_hot) / len(train_y_one_hot)\n",
    "best_val_accuracy = np.sum(best_random_forest.predict(validation_x_one_hot) == validation_y_one_hot) / len(validation_y_one_hot)\n",
    "best_test_accuracy = np.sum(best_random_forest.predict(test_x_one_hot) == test_y_one_hot) / len(validation_y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'best oob score - {best_oob_score}')\n",
    "print(f'train - {best_train_accuracy}')\n",
    "print(f'valid-{best_val_accuracy}')\n",
    "print(f'test-{best_test_accuracy}')\n",
    "print(f'e={best_params[0]}, mf={best_params[1]}, mns={best_params[2]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

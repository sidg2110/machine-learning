{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = pd.read_csv('Corona_train.csv').to_numpy().T\n",
    "\n",
    "m = training_data.shape[1]\n",
    "\n",
    "training_labels = training_data[1].reshape((1, m))\n",
    "training_documents = training_data[2].reshape((1, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CREATING THE VOCABULARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocabulary(training_labels, training_documents):\n",
    "    vocabulary = []\n",
    "    frequency = []    \n",
    "    label_freq = [0, 0, 0]\n",
    "    total_label_len = [0, 0, 0]\n",
    "    vocabulary_dict = {}\n",
    "    labels_dict = {\"Positive\" : 0, \"Negative\": 1, \"Neutral\": 2}\n",
    "\n",
    "    for i in range(m):\n",
    "        label = training_labels[0][i]\n",
    "        doc = training_documents[0][i]\n",
    "        words = doc.split()\n",
    "        \n",
    "        idx = labels_dict[label]\n",
    "        label_freq[idx] += 1\n",
    "        total_label_len[idx] += len(words)\n",
    "        \n",
    "        for word in words:\n",
    "            if word in vocabulary_dict.keys():\n",
    "                word_idx = vocabulary_dict[word]\n",
    "                frequency[word_idx][idx] += 1\n",
    "            else:\n",
    "                vocabulary.append(word)\n",
    "                frequency.append([0, 0, 0])\n",
    "                vocabulary_dict[word] = len(vocabulary)-1\n",
    "                word_idx = vocabulary_dict[word]\n",
    "                frequency[word_idx][idx] += 1\n",
    "                \n",
    "    return (vocabulary, frequency, label_freq, total_label_len, vocabulary_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAIVE BAYES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_bayes(vocabulary, frequency, label_freq, total_label_len, training_labels):\n",
    "    \n",
    "    m = training_labels.shape[1]\n",
    "    words = len(vocabulary)\n",
    "\n",
    "    phi_positive = label_freq[0]/m\n",
    "    phi_negative = label_freq[1]/m\n",
    "    phi_neutral = label_freq[2]/m\n",
    "\n",
    "    phi_params = np.array([[phi_positive, phi_negative, phi_neutral]])\n",
    "\n",
    "    theta_pos, theta_neg, theta_neutral = [], [], []\n",
    "\n",
    "    for i in range(words):\n",
    "        freq = frequency[i]\n",
    "        pos_param = (freq[0] + 1) / (total_label_len[0] + words)\n",
    "        neg_param = (freq[1] + 1) / (total_label_len[1] + words)\n",
    "        neutral_param = (freq[2] + 1) / (total_label_len[2] + words)\n",
    "        theta_pos.append(pos_param)\n",
    "        theta_neg.append(neg_param)\n",
    "        theta_neutral.append(neutral_param)\n",
    "    \n",
    "    theta_pos_params = np.array(theta_pos).reshape((1, words))\n",
    "    theta_neg_params = np.array(theta_neg).reshape((1, words))\n",
    "    theta_neutral_params = np.array(theta_neutral).reshape((1, words))\n",
    "\n",
    "    return (phi_params, theta_pos_params, theta_neg_params, theta_neutral_params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "INFERENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_x_given_y(document, vocabulary_dict, theta_params):\n",
    "    log_prob = 0\n",
    "    words = document.split()\n",
    "    \n",
    "    for word in words:\n",
    "        if word in vocabulary_dict.keys():\n",
    "            word_idx = vocabulary_dict[word]\n",
    "            log_prob += np.log(theta_params[0][word_idx])\n",
    "\n",
    "    return log_prob\n",
    "   \n",
    "def get_prediction(training_documents, vocabulary_dict, phi_params, theta_pos_params, theta_neg_params, theta_neutral_params):\n",
    "    number_docs = training_documents.shape[1]\n",
    "    predictions = []\n",
    "    for i in range(number_docs):\n",
    "        doc = training_documents[0][i]\n",
    "\n",
    "        prob_pos = compute_x_given_y(doc, vocabulary_dict, theta_pos_params) + np.log(phi_params[0][0])\n",
    "        prob_neg = compute_x_given_y(doc, vocabulary_dict, theta_neg_params) + np.log(phi_params[0][1])\n",
    "        prob_neutral = compute_x_given_y(doc, vocabulary_dict, theta_neutral_params) + np.log(phi_params[0][2])\n",
    "\n",
    "        max_value = max(prob_pos, prob_neg, prob_neutral)\n",
    "\n",
    "        if max_value == prob_pos:\n",
    "            predictions.append(\"Positive\")\n",
    "        elif max_value == prob_neg:\n",
    "            predictions.append(\"Negative\")\n",
    "        else:\n",
    "            predictions.append(\"Neutral\")\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8504648214663004"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary, frequency, label_freq, total_label_len, vocabulary_dict = create_vocabulary(training_labels, training_documents)\n",
    "\n",
    "phi_params, theta_pos_params, theta_neg_params, theta_neutral_params = naive_bayes(vocabulary, frequency, label_freq, total_label_len, training_labels)\n",
    "\n",
    "predictions = get_prediction(training_documents, vocabulary_dict, phi_params, theta_pos_params, theta_neg_params, theta_neutral_params)\n",
    "\n",
    "ans=0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == training_labels[0][i]:\n",
    "        ans+=1\n",
    "ans/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6705132098390525"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('Corona_validation.csv').to_numpy().T\n",
    "\n",
    "n = test_data.shape[1]\n",
    "\n",
    "test_labels = test_data[1].reshape((1, n))\n",
    "test_documents = test_data[2].reshape((1, n))\n",
    "\n",
    "predictions = get_prediction(test_documents, vocabulary_dict, phi_params, theta_pos_params, theta_neg_params, theta_neutral_params)\n",
    "\n",
    "ans=0\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == test_labels[0][i]:\n",
    "        ans+=1\n",
    "ans/len(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float('-inf')*(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

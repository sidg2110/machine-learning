{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samples a million training samples from Gaussian distributions\n",
    "# We also add noise to the data - also sampled from a normal distribution\n",
    "def sample_training_set(number_of_samples, theta_params, gaussian_1, gaussian_2, noise_gaussian):\n",
    "    mu_1, var_1 = gaussian_1[0], gaussian_1[1]\n",
    "    mu_2, var_2 = gaussian_2[0], gaussian_2[1]\n",
    "    mu_noise, var_noise = noise_gaussian[0], noise_gaussian[1]\n",
    "\n",
    "    std_1, std_2, std_noise = np.sqrt(var_1), np.sqrt(var_2), np.sqrt(var_noise)\n",
    "\n",
    "    x_1_training_set = np.random.normal(loc=mu_1, scale=std_1, size=(1, number_of_samples))\n",
    "    x_2_training_set = np.random.normal(loc=mu_2, scale=std_2, size=(1, number_of_samples))\n",
    "    noise_training_set = np.random.normal(loc=mu_noise, scale=std_noise, size=(1, number_of_samples))\n",
    "\n",
    "    # Additional row for x_0 feature for each training example\n",
    "    x_0 = np.ones((1, number_of_samples))\n",
    "\n",
    "    # Final input and output training dataset before shuffling\n",
    "    training_set_x = np.vstack((x_0, x_1_training_set, x_2_training_set))\n",
    "    training_set_y = np.dot(theta_params, training_set_x) + noise_training_set\n",
    "\n",
    "    # Shuffling the data set to remove any inherent bias\n",
    "    training_set = np.vstack((training_set_x, training_set_y))\n",
    "    training_set = training_set.T\n",
    "    np.random.shuffle(training_set)\n",
    "    training_set = training_set.T\n",
    "    # Final input and output training dataset after shuffling\n",
    "    training_set_x = training_set[0:3, :]\n",
    "    training_set_y = training_set[3:4, :]\n",
    "\n",
    "    return (training_set_x, training_set_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPLEMENTATION OF STOCHASTIC GRADIENT DESCENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes the 'least mean square' cost function value for the training set\n",
    "def get_cost_value(training_set_x, training_set_y, learning_params):\n",
    "    batch_size = training_set_x.shape[1]\n",
    "    # print(training_set_x.shape)\n",
    "    return (1/(2*batch_size)) * np.sum(np.square(training_set_y - np.dot(learning_params, training_set_x)))\n",
    "\n",
    "# Implementation of the stochastic gradient descent algorithm to learn the theta parameters\n",
    "def stochastic_gradient_descent(training_set_x, training_set_y, learning_rate, batch_size, allowed_error):\n",
    "    \n",
    "    number_of_training_samples = training_set_x.shape[1]\n",
    "    \n",
    "    number_of_batches = int (number_of_training_samples/batch_size)\n",
    "    learning_params = np.zeros((1, 3))\n",
    "    iteration = 0\n",
    "\n",
    "    converged = False\n",
    "\n",
    "    # Stores the values of the cost function of each batch in the last epoch\n",
    "    batch_cost_values_prev = np.zeros(number_of_batches)\n",
    "    # Stores the values of the cost function of each batch in the current epoch\n",
    "    batch_cost_values_new = np.zeros(number_of_batches)\n",
    "    \n",
    "    # Stores the values of theta parameters at each iteration\n",
    "    theta_params_list = np.array([[0, 0, 0]])\n",
    "    \n",
    "    while not converged:\n",
    "        \n",
    "        # Computes which batch is currently processed (0-indexed)\n",
    "        batch = iteration % number_of_batches\n",
    "        \n",
    "        iteration = iteration + 1\n",
    "\n",
    "        # Start and end index of training set examples present in the current batch (0-indexed)\n",
    "        start_idx = (batch) * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "\n",
    "        # Slices the complete training set to get the batches\n",
    "        x_batch = training_set_x[:, start_idx : end_idx]\n",
    "        y_batch = training_set_y[:, start_idx : end_idx]\n",
    "\n",
    "        # Only during the first epoch, stores the values of cost function.\n",
    "        # Next epoch onwards, batch_cost_values_prev =  batch_cost_values_new of the last epoch\n",
    "        if iteration <= number_of_batches:\n",
    "            batch_cost_values_prev[batch] = get_cost_value(x_batch, y_batch, learning_params)\n",
    "        \n",
    "        # Updating theta parameters using gradient descent rule\n",
    "        h_theta_x = np.dot(learning_params, x_batch)\n",
    "        loss_array = x_batch * (y_batch - h_theta_x)\n",
    "        gradient = (1/batch_size) * np.sum(loss_array, axis=1)\n",
    "        learning_params += learning_rate * gradient\n",
    "\n",
    "        theta_params_list = np.append(theta_params_list, learning_params, axis=0)\n",
    "        batch_cost_values_new[batch] = get_cost_value(x_batch, y_batch, learning_params)\n",
    "\n",
    "        # Check for convergence\n",
    "        # Happens at the end of each epoch\n",
    "        # In case the algorithm oscillates about the minima, the convergence is decided by number of iterations.\n",
    "        if ((iteration % number_of_batches == 0) or (iteration > 100000)):\n",
    "            \n",
    "            # Calculates the average of cost values over all the batches for the last and current epoch\n",
    "            prev_avg = np.average(batch_cost_values_prev)\n",
    "            new_avg = np.average(batch_cost_values_new)\n",
    "            if ((abs(new_avg - prev_avg) <= allowed_error) or (iteration > 100000)):\n",
    "                converged = True\n",
    "            \n",
    "            # Updates batch_cost_values_prev\n",
    "            batch_cost_values_prev = batch_cost_values_new.copy()\n",
    "\n",
    "    return (learning_params, iteration, theta_params_list.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS FOR PLOTTING THE GRAPHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_theta_params(theta_0_values, theta_1_values, theta_2_values, plot_axes):\n",
    "    plot_axes.scatter(theta_0_values, theta_1_values, theta_2_values, c='r', linewidth=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SAMPLING THE TRAINING DATASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the 1 million training examples\n",
    "number_of_samples = 1000000\n",
    "actual_theta_params = np.array([[3, 1, 2]])\n",
    "gaussian_1, gaussian_2, noise_gaussian = [3, 4], [-1, 4], [0, 2]\n",
    "training_set_x, training_set_y = sample_training_set(number_of_samples, actual_theta_params, gaussian_1, gaussian_2, noise_gaussian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORTING TEST DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_x = pd.read_csv('q2test.csv')\n",
    "\n",
    "test_set = dataframe_x.to_numpy().T\n",
    "\n",
    "number_of_test_examples  = test_set.shape[1]\n",
    "\n",
    "x_1_features = test_set[0].reshape((1, number_of_test_examples))\n",
    "x_2_features = test_set[1].reshape((1, number_of_test_examples))\n",
    "\n",
    "x_0_features = np.zeros((1, number_of_test_examples))\n",
    "\n",
    "test_set_x = np.vstack((x_0_features, x_1_features, x_2_features))\n",
    "test_set_y = test_set[2].reshape((1, number_of_test_examples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL AND COMPUTING COST ON TEST DATASET - Batch Size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Batch Size = 1\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m theta_params_1, iterations_1, theta_params_list_1 \u001b[39m=\u001b[39m stochastic_gradient_descent(training_set_x, training_set_y, \u001b[39m0.001\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m1e-3\u001b[39;49m)\n\u001b[0;32m      4\u001b[0m cost_on_training_set_1 \u001b[39m=\u001b[39m get_cost_value(training_set_x, training_set_y, theta_params_1)\n\u001b[0;32m      6\u001b[0m cost_on_test_set_1 \u001b[39m=\u001b[39m get_cost_value(test_set_x, test_set_y, theta_params_1)\n",
      "Cell \u001b[1;32mIn[3], line 52\u001b[0m, in \u001b[0;36mstochastic_gradient_descent\u001b[1;34m(training_set_x, training_set_y, learning_rate, batch_size, allowed_error)\u001b[0m\n\u001b[0;32m     49\u001b[0m gradient \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m\u001b[39m/\u001b[39mbatch_size) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39msum(loss_array, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     50\u001b[0m learning_params \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m learning_rate \u001b[39m*\u001b[39m gradient\n\u001b[1;32m---> 52\u001b[0m theta_params_list \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mappend(theta_params_list, learning_params, axis\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     53\u001b[0m batch_cost_values_new[batch] \u001b[39m=\u001b[39m get_cost_value(x_batch, y_batch, learning_params)\n\u001b[0;32m     55\u001b[0m \u001b[39m# Check for convergence\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[39m# Happens at the end of each epoch\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[39m# In case the algorithm oscillates about the minima, the convergence is decided by number of iterations.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ee121\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\numpy\\lib\\function_base.py:5617\u001b[0m, in \u001b[0;36mappend\u001b[1;34m(arr, values, axis)\u001b[0m\n\u001b[0;32m   5615\u001b[0m     values \u001b[39m=\u001b[39m ravel(values)\n\u001b[0;32m   5616\u001b[0m     axis \u001b[39m=\u001b[39m arr\u001b[39m.\u001b[39mndim\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 5617\u001b[0m \u001b[39mreturn\u001b[39;00m concatenate((arr, values), axis\u001b[39m=\u001b[39;49maxis)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Batch Size = 1\n",
    "theta_params_1, iterations_1, theta_params_list_1 = stochastic_gradient_descent(training_set_x, training_set_y, 0.001, 1, 1e-3)\n",
    "\n",
    "cost_on_training_set_1 = get_cost_value(training_set_x, training_set_y, theta_params_1)\n",
    "\n",
    "cost_on_test_set_1 = get_cost_value(test_set_x, test_set_y, theta_params_1)\n",
    "\n",
    "theta_params_1, cost_on_training_set_1, cost_on_test_set_1, iterations_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL AND COMPUTING COST ON TEST DATASETL - Batch Size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size = 100\n",
    "theta_params_2, iterations_2, theta_params_list_2 = stochastic_gradient_descent(training_set_x, training_set_y, 0.001, 100, 1e-5)\n",
    "\n",
    "cost_on_training_set_2 = get_cost_value(training_set_x, training_set_y, theta_params_2)\n",
    "\n",
    "cost_on_test_set_2 = get_cost_value(test_set_x, test_set_y, theta_params_2)\n",
    "\n",
    "theta_params_2, cost_on_training_set_2, cost_on_test_set_2, iterations_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL AND COMPUTING COST ON TEST DATASET - Batch Size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size = 10000\n",
    "theta_params_3, iterations_3, theta_params_list_3 = stochastic_gradient_descent(training_set_x, training_set_y, 0.001, 10000, 1e-6)\n",
    "\n",
    "cost_on_training_set_3 = get_cost_value(training_set_x, training_set_y, theta_params_3)\n",
    "\n",
    "cost_on_test_set_3 = get_cost_value(test_set_x, test_set_y, theta_params_3)\n",
    "\n",
    "theta_params_3, cost_on_training_set_3, cost_on_test_set_3, iterations_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL AND COMPUTING COST ON TEST DATASET - Batch Size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Size = 1000000\n",
    "theta_params_4, iterations_4, theta_params_list_4 = stochastic_gradient_descent(training_set_x, training_set_y, 0.001, 1000000, 1e-6)\n",
    "\n",
    "cost_on_training_set_4 = get_cost_value(training_set_x, training_set_y, theta_params_4)\n",
    "\n",
    "cost_on_test_set_4 = get_cost_value(test_set_x, test_set_y, theta_params_4)\n",
    "\n",
    "theta_params_4, cost_on_training_set_4, cost_on_test_set_4, iterations_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ACTUAL HYPOTHESIS OVER THE TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_cost_test_set = get_cost_value(test_set_x, test_set_y, actual_theta_params)\n",
    "actual_cost_test_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING THE SCATTER PLOTS OF THETA PARAMETERS - Batch Size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1 = plt.figure(figsize=(10, 10))\n",
    "plot_1_axes = plot_1.add_axes([0, 0.1, 1, 0.8], projection='3d')\n",
    "plot_1_axes.set_xlabel('theta_0')\n",
    "plot_1_axes.set_ylabel('theta_1')\n",
    "plot_1_axes.set_zlabel('theta_2')\n",
    "plot_1_axes.set_title('Batch Size = 1')\n",
    "plt_theta_params(theta_params_list_1[0], theta_params_list_1[1], theta_params_list_1[2], plot_1_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING THE SCATTER PLOTS OF THETA PARAMETERS - Batch Size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2 = plt.figure(figsize=(10, 10))\n",
    "plot_2_axes = plot_2.add_axes([0.01, 0.1, 1, 0.8], projection='3d')\n",
    "plot_2_axes.set_xlabel('theta_0')\n",
    "plot_2_axes.set_ylabel('theta_1')\n",
    "plot_2_axes.set_zlabel('theta_2')\n",
    "plot_2_axes.set_title('Batch Size = 100')\n",
    "\n",
    "plt_theta_params(theta_params_list_2[0], theta_params_list_2[1], theta_params_list_2[2], plot_2_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING THE SCATTER PLOTS OF THETA PARAMETERS - Batch Size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3 = plt.figure(figsize=(10, 10))\n",
    "plot_3_axes = plot_3.add_axes([0, 0.1, 1, 0.8], projection='3d')\n",
    "plot_3_axes.set_xlabel('theta_0')\n",
    "plot_3_axes.set_ylabel('theta_1')\n",
    "plot_3_axes.set_zlabel('theta_2')\n",
    "plot_3_axes.set_title('Batch Size = 10000')\n",
    "\n",
    "plt_theta_params(theta_params_list_3[0], theta_params_list_3[1], theta_params_list_3[2], plot_3_axes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLOTTING THE SCATTER PLOTS OF THETA PARAMETERS - Batch Size = 1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_4 = plt.figure(figsize=(10, 10))\n",
    "plot_4_axes = plot_4.add_axes([0, 0.1, 1, 0.8], projection='3d')\n",
    "plot_4_axes.set_xlabel('theta_0')\n",
    "plot_4_axes.set_ylabel('theta_1')\n",
    "plot_4_axes.set_zlabel('theta_2')\n",
    "plot_4_axes.set_title('Batch Size = 1000000')\n",
    "\n",
    "plt_theta_params(theta_params_list_4[0], theta_params_list_4[1], theta_params_list_4[2], plot_4_axes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
